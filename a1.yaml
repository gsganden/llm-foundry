name: 134m-draft-cl-10tpr-train
image: mosaicml/llm-foundry:2.4.0_cu124-latest
scheduling:
  priority: medium
  max_retries: 0
  preemptible: true
  retry_on_system_failure: false
compute:
  gpus: 32
  cluster: prod-azure-eastus2-mlserv-0-7yzjpxp1
  instance: azure.Standard_ND96isr_H100_v5
parameters:
  seed: 42
  model:
    name: mpt_causal_lm
    d_model: 640
    n_heads: 10
    no_bias: true
    n_layers: 12
    norm_eps: 1.0e-05
    norm_type: rmsnorm
    use_cache: false
    ffn_config:
      ffn_type: mptglu
      ffn_act_fn:
        name: silu
      ffn_hidden_size: 1792
    vocab_size: 128256
    attn_config:
      rope: true
      alibi: false
      attn_impl: flash
      attn_type: grouped_query_attention
      fused_qkv: false
      rope_impl: hf
      kv_n_heads: 1
      rope_theta: 500000
      rope_hf_config:
        type: llama3
        factor: 8
        low_freq_factor: 1
        high_freq_factor: 4
        original_max_position_embeddings: 8192
      attn_uses_sequence_id: true
    init_device: meta
    max_seq_len: ${max_seq_len}
    bos_token_id: 128000
    eos_token_id: 128001
    learned_pos_emb: false
    use_train_metrics: true
    use_pad_tok_in_ffn: true
    fuse_norm_attn_norm: true
    tie_word_embeddings: true
  loggers:
    mlflow:
      tags:
        run: 134m-draft-cl-10tpr-train
        project: specdec_distillation
      tracking_uri: databricks
      experiment_name: /Users/abhay.gupta@databricks.com/specdec_distillation/
  callbacks:
    lr_monitor: {}
    dataset_swap:
      dataset_index: 1
    scheduled_gc:
      batch_interval: 2000
    speed_monitor:
      window_size: 10
    memory_monitor:
      dist_aggregate_batch_interval: 25
    hf_checkpointer:
      precision: bfloat16
      convert_to: llama
      save_folder: ${save_folder}
      save_interval: ${save_interval}
    runtime_estimator: {}
  load_path: oci://mosaicml-internal-checkpoints/abhay/specdec_distillation/134m-draft-10tpr-train/ep0-ba640
  optimizer:
    lr: 0.0006
    name: decoupled_lionw
    betas:
    - 0.9
    - 0.95
    weight_decay: 0.0006
  precision: amp_bf16
  scheduler:
    name: cosine_with_warmup
    alpha_f: 0.1
    t_warmup: 8388608000tok
  tokenizer:
    name: meta-llama/Llama-3.1-8B
    kwargs:
      model_max_length: ${max_seq_len}
  algorithms:
    gradient_clipping:
      clipping_type: norm
      clipping_threshold: 1
  autoresume: true
  eval_first: false
  fsdp_config:
    verbose: false
    mixed_precision: PURE
    state_dict_type: sharded
    forward_prefetch: true
    backward_prefetch: BACKWARD_PRE
    limit_all_gathers: true
    sharding_strategy: FULL_SHARD
    activation_cpu_offload: false
    activation_checkpointing: false
    activation_checkpointing_reentrant: false
  max_seq_len: 32768
  save_folder: oci://mosaicml-internal-checkpoints/abhay/specdec_distillation/134m-draft-cl-10tpr-train
  dist_timeout: 1800
  max_duration: 1342003200tok
  progress_bar: false
  train_loader:
    name: text
    dataset:
      shuffle: true
      streams:
        cc:
          local: /mnt/data/datasets/cc
          split: null
          remote: dbfs:/Volumes/datasets/saaketh/quantization/llama3_1_final_cc_32k
          proportion: 0.05
        hq:
          local: /mnt/data/datasets/hq
          split: null
          remote: dbfs:/Volumes/datasets/saaketh/quantization/llama3_1_final_hq_32k
          proportion: 0.35
        code:
          local: /mnt/data/datasets/code
          split: null
          remote: dbfs:/Volumes/datasets/saaketh/quantization/llama3_1_final_code_32k
          proportion: 0.35
        fineweb:
          local: /mnt/data/dataset/fineweb/
          split: null
          remote: dbfs:/Volumes/datasets/saaketh/quantization/llama3_1_final_fineweb_32k
          proportion: 0.25
      cache_limit: 20tb
      max_seq_len: ${max_seq_len}
      eos_token_id: 128001
      shuffle_algo: py1e
      shuffle_seed: ${seed}
      download_timeout: 360
      shuffle_block_size: 25000
      num_canonical_nodes: 4
    drop_last: true
    num_workers: 8
  eval_interval: 0
  save_interval: 1dur
  log_to_console: true
  console_log_interval: 100ba
  device_eval_batch_size: 1
  global_train_batch_size: 128
  device_train_microbatch_size: 1
  save_num_checkpoints_to_keep: 1
integrations:
- integration_type: git_repo
  path: /workspace/llm-foundry
  git_repo: mosaicml/llm-foundry
  ssh_clone: false
  git_branch: main
  pip_install: .[gpu,openai]
- integration_type: git_repo
  git_repo: databricks-mosaic/runtime-private-plugins
  ssh_clone: true
  git_branch: main
  pip_install: .[gpu]
command: |-
  export AWS_PROFILE=data-force-one
  export MOSAICML_PLATFORM=TRUE
  export CUDA_MODULE_LOADING=LAZY
  export PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True'
  export TQDM_DISABLE=1

  cd /workspace/llm-foundry/scripts
  composer train/train.py /mnt/config/parameters.yaml || (echo "Command failed - killing python" && pkill python && exit 1)
  echo exit code is $?
